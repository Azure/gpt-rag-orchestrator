# Evaluation Process

This document provides an overview of the evaluation process for the GPT-RAG Orchestrator.

## Overview

The evaluation process leverages the Azure AI Project client library to provide quantitative, AI-assisted quality and safety metrics. These metrics are used to assess the performance of LLM models, GenAI applications, and agents. Metrics are defined as evaluators, which can be either built-in or custom, offering comprehensive evaluation insights.

For more details, refer to:
-  [Azure AI Projects Evaluation Documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/cloud-evaluation).
-  [Azure AI Projects Evaluation Python SDK](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/ai/azure-ai-projects/README.md#evaluation).

